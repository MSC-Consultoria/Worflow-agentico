{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colab: Treinamento de Embeddings do Repositório\n",
    "\n",
    "Notebook para rodar no Google Colab um pipeline completo de preparação de dados, fine-tuning de embeddings e publicação no Hugging Face Hub. Configure um runtime com GPU (p.ex., T4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Variáveis de ambiente / segredos\n",
    "Defina no Colab (\"Manage sessions -> Environment variables\" ou via `os.environ`) antes de executar:\n",
    "- `HF_TOKEN`: token do Hugging Face com permissão de write.\n",
    "- `HF_REPO_ID`: ex. `org/nome-modelo-embeddings`.\n",
    "- `HF_DATASET_ID`: ex. `org/nome-dataset-embeddings`.\n",
    "- `GIT_REPO_URL`: URL HTTPS do repo (default: este repositório)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sentence-transformers datasets huggingface_hub gitpython tiktoken\n",
    "!pip install -q evaluate" 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pathlib, json, textwrap\n",
    "from datasets import Dataset, load_dataset\n",
    "from huggingface_hub import HfApi, HfFolder\n",
    "from sentence_transformers import SentenceTransformer, losses, util, models\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import git\n",
    "\n",
    "HF_TOKEN = os.environ.get('HF_TOKEN')\n",
    "HF_REPO_ID = os.environ.get('HF_REPO_ID', 'org/nome-modelo-embeddings')\n",
    "HF_DATASET_ID = os.environ.get('HF_DATASET_ID', 'org/nome-dataset-embeddings')\n",
    "GIT_REPO_URL = os.environ.get('GIT_REPO_URL', 'https://github.com/example/repo.git')\n",
    "BASE_DIR = pathlib.Path('/content/repo')\n",
    "DATA_DIR = pathlib.Path('/content/data')\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "assert HF_TOKEN, 'Defina HF_TOKEN no ambiente.'\n",
    "HfFolder.save_token(HF_TOKEN)\n",
    "api = HfApi(token=HF_TOKEN)\n",
    "print('Autenticado no HF Hub como', api.whoami()['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not BASE_DIR.exists():\n",
    "    print('Clonando repositório...')\n",
    "    git.Repo.clone_from(GIT_REPO_URL, BASE_DIR)\n",
    "else:\n",
    "    print('Repositório já existe, atualizando...')\n",
    "    repo = git.Repo(BASE_DIR)\n",
    "    repo.remotes.origin.pull()\n",
    "\n",
    "def load_texts(root: pathlib.Path):\n",
    "    exts = {'.md', '.txt', '.py', '.js', '.ts', '.rs', '.java'}\n",
    "    for path in root.rglob('*'):\n",
    "        if path.suffix.lower() in exts and path.is_file():\n",
    "            try:\n",
    "                text = path.read_text(encoding='utf-8')\n",
    "            except Exception:\n",
    "                continue\n",
    "            yield {'path': str(path), 'text': text}\n",
    "\n",
    "raw_samples = list(load_texts(BASE_DIR))\n",
    "print('Total de arquivos carregados:', len(raw_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_len=512):\n",
    "    import re\n",
    "    sentences = re.split(r'(?:\\. |\\n)', text)\n    ",
    "    buffer, chunks = [], []\n",
    "    for sent in sentences:\n",
    "        if len(' '.join(buffer + [sent])) > max_len:\n",
    "            if buffer:\n",
    "                chunks.append(' '.join(buffer))\n",
    "                buffer = []\n",
    "        buffer.append(sent)\n",
    "    if buffer:\n",
    "        chunks.append(' '.join(buffer))\n",
    "    return [c.strip() for c in chunks if len(c.strip()) > 50]\n",
    "\n",
    "chunked = []\n",
    "for sample in raw_samples:\n",
    "    for c in chunk_text(sample['text']):\n",
    "        chunked.append({'text': c, 'source': sample['path']})\n",
    "\n",
    "print('Total de chunks:', len(chunked))\n",
    "dataset = Dataset.from_list(chunked)\n",
    "dataset.push_to_hub(HF_DATASET_ID, token=HF_TOKEN, private=True)\n",
    "dataset" 
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'sentence-transformers/all-MiniLM-L12-v2'\n",
    "model = SentenceTransformer(model_name)\n",
    "train_samples = [{'texts': [t, t]} for t in dataset['text']]\n",
    "train_dataset = Dataset.from_list(train_samples)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "epochs = 1\n",
    "warmup_steps = int(len(train_dataloader) * epochs * 0.1)\n",
    "\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=epochs,\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path='/content/model-output'\n",
    ")\n",
    "\n",
    "api.upload_folder(\n",
    "    folder_path='/content/model-output',\n",
    "    repo_id=HF_REPO_ID,\n",
    "    token=HF_TOKEN,\n",
    "    repo_type='model',\n",
    "    commit_message='Upload embedding model (colab training)'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliação simples (similaridade)\n",
    "pairs = [\n",
    "    (dataset['text'][0], dataset['text'][1] if len(dataset) > 1 else dataset['text'][0]),\n",
    "]\n",
    "emb = model.encode([p for pair in pairs for p in pair], convert_to_tensor=True)\n",
    "scores = util.pytorch_cos_sim(emb[0::2], emb[1::2])\n",
    "print('Cosine similarity (amostra):', scores.mean().item())\n",
    "\n",
    "# Registro de artefatos locais (opcional)\n",
    "with open('/content/run_log.json', 'w') as f:\n",
    "    json.dump({'similarity_mean': scores.mean().item(), 'total_chunks': len(dataset)}, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
